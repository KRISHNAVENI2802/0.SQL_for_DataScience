{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.PySpark Connection part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connection part for Pyspark and importing required packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Pyspark_VS_Pandas\").getOrCreate()\n",
    "conf = spark.sparkContext._conf.setAll([('spark.driver.memory', '4g'), ('spark.executor.memory', '4g'), ('spark.executor.num','6'), ('spark.network.timeout', '1000000')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Reading data into Pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Pyspark dataframe so that we apply SQL scripts for our practice\n",
    "#    1.Read it into Pandas df\n",
    "#    2.convert into pyspark df by defining datatypes of each columns\n",
    "#    [we can use spark.read.format(\"\") option, but it requires additional packages installation so skipped this way]\n",
    "\n",
    "\n",
    "student_dfpd = pd.read_excel(r'Table_Source\\Student_Placement_Table.xlsx')\n",
    "schema = StructType([\\\n",
    "                     StructField(\"ID\",IntegerType(),False),\\\n",
    "                     StructField(\"Name\",StringType(),False),\\\n",
    "                     StructField(\"Gender\",StringType(),False),\\\n",
    "                     StructField(\"DOB\",DateType(),False),\\\n",
    "                     StructField(\"Location\",StringType(),True),\\\n",
    "                     StructField(\"University\",StringType(),False),\\\n",
    "                     StructField(\"Salary\",DoubleType(),False),\\\n",
    "                     StructField(\"Company\",StringType(),False)])\n",
    "\n",
    "student_dfps = spark.createDataFrame(student_dfpd,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.PySpark DataFrame to TempView + Columns datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- Name: string (nullable = false)\n",
      " |-- Gender: string (nullable = false)\n",
      " |-- DOB: date (nullable = false)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- University: string (nullable = false)\n",
      " |-- Salary: double (nullable = false)\n",
      " |-- Company: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A. Create pyspark dataframe into Temporary view for applying SQL scripts\n",
    "student_dfps.createOrReplaceTempView(\"Student_Table\")\n",
    "\n",
    "#B. Checking the datatypes of columns\n",
    "student_dfps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records of Student_Table =  24\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|\n",
      "|107| GGG|     F|1994-10-10|  Mumbai|       VIT|60200.7|   Amazon|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|\n",
      "|109| III|     F|1994-12-21|     HYD|      IIIT|66980.8|   Google|\n",
      "|110| JJJ|     M|1990-11-22| Chennai|       NIT|59250.2|   Amazon|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|\n",
      "|112| LLL|     F|1995-09-20|  Mumbai|       NIT|67900.8|    Apple|\n",
      "|113| MMM|     F|1994-10-15|     HYD|       VIT|60200.7|   Google|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|\n",
      "|116| PPP|     F|1994-10-28| Chennai|       IIT|59050.5|    Apple|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|\n",
      "|118| RRR|     F|1993-11-10|  Mumbai|       NIT|52900.5|   Google|\n",
      "|119| SSS|     M|1992-10-25|     HYD|       VIT|62900.5|Microsoft|\n",
      "|120| TTT|     M|1995-09-29| Chennai|      IIIT|57230.5|    Apple|\n",
      "|121| UUU|     F|1990-11-13| Chennai|       NIT|59250.2|   Google|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|\n",
      "|123| WWW|     F|1994-09-14|  Mumbai|       VIT|59050.5|    Apple|\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print total count of records \n",
    "print(\"Total records of Student_Table = \",student_dfps.count())\n",
    "\n",
    "#List all the records in table\n",
    "sql_query = \"SELECT * FROM Student_Table\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.SQL Practice starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @##############################################################@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4A.Select statement + Alias names + Limit + Count(*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A1). Print only ID, NAME, GENDER columns\n",
      "+---+----+------+\n",
      "| ID|NAME|GENDER|\n",
      "+---+----+------+\n",
      "|101| AAA|     M|\n",
      "|102| BBB|     F|\n",
      "|103| CCC|     M|\n",
      "|104| DDD|     F|\n",
      "|105| EEE|     M|\n",
      "+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select statement used to select(Print) data \n",
    "#We can give perticular column names to print, or use * to print all columns\n",
    "# Table name = Student_Table\n",
    "#show(5) for limiting records tobe printed\n",
    "\n",
    "print(\"4A1). Print only ID, NAME, GENDER columns\")\n",
    "sql_query=\"\"\"SELECT ID, NAME, GENDER FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A2). Print all columns from table\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use * for printing all columns data to console, ##show(5) for limiting records tobe printed\n",
    "print(\"4A2). Print all columns from table\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A3). Alias name for ID, Name columns\n",
      "+---------+---------------+\n",
      "|ID_Number|Name_of_Student|\n",
      "+---------+---------------+\n",
      "|      101|            AAA|\n",
      "|      102|            BBB|\n",
      "|      103|            CCC|\n",
      "|      104|            DDD|\n",
      "|      105|            EEE|\n",
      "+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming columns with more meaningful names\n",
    "print(\"4A3). Alias name for ID, Name columns\")\n",
    "sql_query=\"\"\"SELECT ID as ID_Number, Name as Name_of_Student FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A4). Limiting number of records tobe printing on console with Limit by 4\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Limiting number of records with LIMIT\n",
    "print(\"4A4). Limiting number of records tobe printing on console with Limit by 4\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table LIMIT 4\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A5). Print total records in given table\n",
      "+-----------+\n",
      "|Total_Count|\n",
      "+-----------+\n",
      "|         24|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count(*) function used to return total number of records that are matching given criteria\n",
    "    #if no filter given for count(*), it will print total records in given table\n",
    "\n",
    "print(\"4A5). Print total records in given table\")\n",
    "sql_query=\"\"\"SELECT count(*)as Total_Count FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A6). Print some sample text using select statement\n",
      "+--------------+\n",
      "|   Column_Name|\n",
      "+--------------+\n",
      "|Hello I am SQL|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selectng some random text using select statement\n",
    "\n",
    "print(\"4A6). Print some sample text using select statement\")\n",
    "sql_query=\"\"\"SELECT 'Hello I am SQL' as Column_Name \"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4B.Distinct statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4B1). Without Distinct statement, it will lsit all reocrds in that column(s)\n",
      "+--------+\n",
      "|Location|\n",
      "+--------+\n",
      "|Banglore|\n",
      "|     HYD|\n",
      "| Chennai|\n",
      "|  Mumbai|\n",
      "| Chennai|\n",
      "|     HYD|\n",
      "|  Mumbai|\n",
      "|Banglore|\n",
      "|     HYD|\n",
      "| Chennai|\n",
      "|Banglore|\n",
      "|  Mumbai|\n",
      "|     HYD|\n",
      "|Banglore|\n",
      "|Banglore|\n",
      "| Chennai|\n",
      "|Banglore|\n",
      "|  Mumbai|\n",
      "|     HYD|\n",
      "| Chennai|\n",
      "| Chennai|\n",
      "|Banglore|\n",
      "|  Mumbai|\n",
      "|     HYD|\n",
      "+--------+\n",
      "\n",
      "4B2). With Distinct statement, it will lsit only distinct reocrds in that column(s)\n",
      "+--------+\n",
      "|Location|\n",
      "+--------+\n",
      "| Chennai|\n",
      "|  Mumbai|\n",
      "|     HYD|\n",
      "|Banglore|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distinct statement used for listing only distinct(different) values in column or list of columns\n",
    "print(\"4B1). Without Distinct statement, it will lsit all reocrds in that column(s)\")\n",
    "sql_query=\"SELECT Location FROM Student_Table\"\n",
    "spark.sql(sql_query).show(30)\n",
    "\n",
    "print(\"4B2). With Distinct statement, it will lsit only distinct reocrds in that column(s)\")\n",
    "sql_query=\"SELECT distinct Location FROM Student_Table\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4C.WHERE clause + BETWEEN + LIKE + IN + AND + OR + IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C1). Print records only from Banglore location\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Where clause used to filter reocrds based on given condition on columns\n",
    "#Below query will filter records from location Banglore\n",
    "\n",
    "print(\"4C1). Print records only from Banglore location\")\n",
    "sql_query=\"SELECT * FROM Student_Table WHERE Location = 'Banglore'\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C2). Print records only ID range from 105 to 109 Inclusive\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|\n",
      "|107| GGG|     F|1994-10-10|  Mumbai|       VIT|60200.7|   Amazon|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|\n",
      "|109| III|     F|1994-12-21|     HYD|      IIIT|66980.8|   Google|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4C2). Print records only ID range from 105 to 109 Inclusive\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE ID BETWEEN 105 AND 109\"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C3). Print records only Company value contains soft\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|\n",
      "|119| SSS|     M|1992-10-25|     HYD|       VIT|62900.5|Microsoft|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4C3). Print records only Company value contains soft\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE COMPANY LIKE \"%soft%\" \"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C4). Print records only Name in given list(AAA, GGG, KKK)\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|107| GGG|     F|1994-10-10|  Mumbai|       VIT|60200.7|   Amazon|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4C4). Print records only Name in given list(AAA, GGG, KKK)\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE NAME IN ('AAA', 'GGG', 'KKK') \"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C5). Print records from Banglore location and Microsoft company\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#And should satisfy all conditions\n",
    "print(\"4C5). Print records from Banglore location and Microsoft company\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE (LOCATION ='Banglore' AND COMPANY ='Microsoft') \"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C6). Print records from Banglore location or Microsoft company\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|\n",
      "|119| SSS|     M|1992-10-25|     HYD|       VIT|62900.5|Microsoft|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OR should satisfy any one conditions, Either of the condition will meet the output\n",
    "print(\"4C6). Print records from Banglore location or Microsoft company\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE (LOCATION ='Banglore' OR COMPANY ='Microsoft') \"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4C7). Print records where University is not null, Here all records will be printed because all rows are having Proper values in University column\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|102| BBB|     F|1995-09-20|     HYD|      IIIT|76000.2|   Amazon|\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|\n",
      "|105| EEE|     M|1993-05-19| Chennai|       IIT|60200.7|Microsoft|\n",
      "|106| FFF|     M|1994-07-23|     HYD|      IIIT|63100.8|Microsoft|\n",
      "|107| GGG|     F|1994-10-10|  Mumbai|       VIT|60200.7|   Amazon|\n",
      "|108| HHH|     M|1990-10-10|Banglore|       NIT|89200.7|    Apple|\n",
      "|109| III|     F|1994-12-21|     HYD|      IIIT|66980.8|   Google|\n",
      "|110| JJJ|     M|1990-11-22| Chennai|       NIT|59250.2|   Amazon|\n",
      "|111| KKK|     M|1994-10-10|Banglore|      IISC|76300.9|Microsoft|\n",
      "|112| LLL|     F|1995-09-20|  Mumbai|       NIT|67900.8|    Apple|\n",
      "|113| MMM|     F|1994-10-15|     HYD|       VIT|60200.7|   Google|\n",
      "|114| NNN|     F|1990-11-29|Banglore|      IIIT|59200.5|   Amazon|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|\n",
      "|116| PPP|     F|1994-10-28| Chennai|       IIT|59050.5|    Apple|\n",
      "|117| QQQ|     M|1991-02-10|Banglore|      IISC|60200.7|Microsoft|\n",
      "|118| RRR|     F|1993-11-10|  Mumbai|       NIT|52900.5|   Google|\n",
      "|119| SSS|     M|1992-10-25|     HYD|       VIT|62900.5|Microsoft|\n",
      "|120| TTT|     M|1995-09-29| Chennai|      IIIT|57230.5|    Apple|\n",
      "|121| UUU|     F|1990-11-13| Chennai|       NIT|59250.2|   Google|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|\n",
      "|123| WWW|     F|1994-09-14|  Mumbai|       VIT|59050.5|    Apple|\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Is Null will filter given column having Null values, #Is Not Null will print having proper value\n",
    "# Null is nothing but, missing value in any column(Except Primay key column), it will represent with some meaningful value\n",
    "print(\"4C7). Print records where University is not null, Here all records will be printed because all rows are having Proper values in University column\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table WHERE University IS NOT NULL\"\"\"\n",
    "spark.sql(sql_query).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4D.Order By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4D1). Sort by Salary Accending order top 5 records\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|103| CCC|     M|1992-12-31| Chennai|       NIT|49200.5|   Google|\n",
      "|118| RRR|     F|1993-11-10|  Mumbai|       NIT|52900.5|   Google|\n",
      "|104| DDD|     F|1990-11-22|  Mumbai|       VIT|54980.6|    Apple|\n",
      "|101| AAA|     M|1994-10-10|Banglore|      IISC|55000.5|Microsoft|\n",
      "|115| OOO|     M|1995-09-20|Banglore|      IISC|57120.5|   Google|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bydefault it is Accending order, for descending order we have to use DESC keyword\n",
    "    #Number: 0 to n bydefault, DESC: n to 0\n",
    "    #Alphabets: A to Z bydefault, DESC: Z to A\n",
    "print(\"4D1). Sort by Salary Accending order top 5 records\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table ORDER BY Salary LIMIT 5\"\"\"\n",
    "spark.sql(sql_query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4D2). Sort by Name Descending order top 5 records\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "| ID|Name|Gender|       DOB|Location|University| Salary|  Company|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "|124| XXX|     M|1991-12-19|     HYD|       IIT|60200.7|Microsoft|\n",
      "|123| WWW|     F|1994-09-14|  Mumbai|       VIT|59050.5|    Apple|\n",
      "|122| VVV|     M|1993-08-19|Banglore|      IISC|57120.5|Microsoft|\n",
      "|121| UUU|     F|1990-11-13| Chennai|       NIT|59250.2|   Google|\n",
      "|120| TTT|     M|1995-09-29| Chennai|      IIIT|57230.5|    Apple|\n",
      "+---+----+------+----------+--------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4D2). Sort by Name Descending order top 5 records\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table ORDER BY Name DESC LIMIT 5\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4E. Upper() + Lower() + Length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4E1). Apply Upper(), Lower(), Length() functions to columns\n",
      "+---------+--------------+--------------+---------------+\n",
      "|  COMPANY|upper(COMPANY)|lower(COMPANY)|length(COMPANY)|\n",
      "+---------+--------------+--------------+---------------+\n",
      "|Microsoft|     MICROSOFT|     microsoft|              9|\n",
      "|    Apple|         APPLE|         apple|              5|\n",
      "|   Amazon|        AMAZON|        amazon|              6|\n",
      "|   Google|        GOOGLE|        google|              6|\n",
      "+---------+--------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Upper()-->Convert given column data into upper case data\n",
    "#Lower()-->Convert given column data into lower case data\n",
    "#Length()-->It will print total characters in columns data including spaces\n",
    "\n",
    "print(\"4E1). Apply Upper(), Lower(), Length() functions to columns\")\n",
    "sql_query=\"\"\"SELECT DISTINCT COMPANY,UPPER(COMPANY), LOWER(COMPANY), LENGTH(COMPANY) FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4F. Concatination(||) + BooleanExpression + TRIM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4F1). Concatination using || symbol\n",
      "+------------------+\n",
      "|         SelfIntro|\n",
      "+------------------+\n",
      "|I am AAA from IISC|\n",
      "|I am BBB from IIIT|\n",
      "| I am CCC from NIT|\n",
      "| I am DDD from VIT|\n",
      "| I am EEE from IIT|\n",
      "|I am FFF from IIIT|\n",
      "| I am GGG from VIT|\n",
      "| I am HHH from NIT|\n",
      "|I am III from IIIT|\n",
      "| I am JJJ from NIT|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Concatination using ||\n",
    "    #This will help to club mutiple columns and some text into single column\n",
    "\n",
    "print(\"4F1). Concatination using || symbol\")\n",
    "sql_query=\"\"\"SELECT 'I am ' || Name || ' from ' || University as SelfIntro FROM Student_Table LIMIT 10\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4F2). Boolean Expression with some condition\n",
      "+---+----+-------+---------------------+\n",
      "| ID|NAME| SALARY|IsSalaryGraterThan60K|\n",
      "+---+----+-------+---------------------+\n",
      "|101| AAA|55000.5|                false|\n",
      "|102| BBB|76000.2|                 true|\n",
      "|103| CCC|49200.5|                false|\n",
      "|104| DDD|54980.6|                false|\n",
      "|105| EEE|60200.7|                 true|\n",
      "|106| FFF|63100.8|                 true|\n",
      "|107| GGG|60200.7|                 true|\n",
      "|108| HHH|89200.7|                 true|\n",
      "|109| III|66980.8|                 true|\n",
      "|110| JJJ|59250.2|                false|\n",
      "+---+----+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Boolean Expression with some condition:\n",
    "    #THis will print True or False values \n",
    "\n",
    "print(\"4F2). Boolean Expression with some condition\")\n",
    "sql_query=\"\"\"SELECT ID, NAME, SALARY, (Salary > 60000) As IsSalaryGraterThan60K FROM Student_Table LIMIT 10\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4F3). Trim() function used to remove extra spaces in column's data\n",
      "+-------------+---------------+-----------+---------------+\n",
      "|  ExtraSpaces|Len_ExtraSpaces|TrimApplied|Len_TrimApplied|\n",
      "+-------------+---------------+-----------+---------------+\n",
      "|   Google    |             13|     Google|              6|\n",
      "+-------------+---------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trim() function used to remove extra spaces in column's data\n",
    "\n",
    "print(\"4F3). Trim() function used to remove extra spaces in column's data\")\n",
    "\n",
    "sql_query=\"\"\"SELECT \n",
    "'   Google    ' AS ExtraSpaces, LENGTH('   Google    ') AS Len_ExtraSpaces,\n",
    "TRIM('   Google    ') AS TrimApplied, LENGTH(TRIM('   Google    ')) AS Len_TrimApplied\n",
    "\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4).\")\n",
    "sql_query=\"\"\"SELECT * FROM Student_Table\"\"\"\n",
    "spark.sql(sql_query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o44.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.crealytics.spark.excel. Please find packages at http://spark.apache.org/third-party-projects.html\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:194)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\r\n\tat scala.util.Try.orElse(Try.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\r\n\t... 13 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2a576e027b71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mStudent_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"com.crealytics.spark.excel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"useHeader\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inferSchema\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"false\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Student_Placement_Table.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o44.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.crealytics.spark.excel. Please find packages at http://spark.apache.org/third-party-projects.html\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:194)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: com.crealytics.spark.excel.DefaultSource\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\r\n\tat scala.util.Try.orElse(Try.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\r\n\t... 13 more\r\n"
     ]
    }
   ],
   "source": [
    "Student_df = spark.read.format(\"com.crealytics.spark.excel\").option(\"useHeader\",\"true\").option(\"inferSchema\",\"false\").load(\"Student_Placement_Table.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python dict\n",
    "dict1 = {\"Name\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\"],\\\n",
    "         \"Weight\":[70,61,83,60,92,69,84,71,77],\\\n",
    "         \"Address\":[\"HYD\",\"Banglore\",\"Chennai\",\"Mumbai\",\"Banglore\",\"Mumbai\",\"Chennai\",\"Banglore\",\"HYD\"],\\\n",
    "         \"DOB\":[\"15-01-1990\", \"19-01-1996\", \"28-02-1999\", \"13-06-1989\", \"15-11-2000\", \"10-12-1995\", \"25-11-1998\", \"15-09-1994\", \"15-01-1996\"],\\\n",
    "         \"Batch\":[2016, 2017, 2018, 2016, 2016, 2017, 2016, 2018, 2017],\\\n",
    "         \"Salary\":[51000.00, 46500.50, 52000.00, 51000.00, 52000.00, 75000.60, 64000.50, 52000.00, 46500.50]         \n",
    "        }\n",
    "\n",
    "#create pandas df\n",
    "dfpd = pd.DataFrame(dict1)\n",
    "dfpd_dtype = {\"Name\":'str', \"Weight\":'int64', \"Address\":'str', \"DOB\":'datetime64', \"Batch\":'int64', \"Salary\":'float64' }\n",
    "dfpd = dfpd.astype(dfpd_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema for spark dataframe with Structtype, fields\n",
    "schema = StructType([\\\n",
    "                     StructField(\"Name\", StringType(), True),\\\n",
    "                     StructField(\"Weight\", IntegerType(), True),\\\n",
    "                     StructField(\"Address\", StringType(), True),\\\n",
    "                     StructField(\"DOB\", DateType(), True),\\\n",
    "                     StructField(\"Batch\", IntegerType(), True),\\\n",
    "                     StructField(\"Salary\", DoubleType(), True)])\n",
    "\n",
    "#create spark DF by passing pandas df with above schema\n",
    "dfps = spark.createDataFrame(dfpd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+----------+-----+-------+\n",
      "|Name|Weight| Address|       DOB|Batch| Salary|\n",
      "+----+------+--------+----------+-----+-------+\n",
      "|   A|    70|     HYD|1990-01-15| 2016|51000.0|\n",
      "|   B|    61|Banglore|1996-01-19| 2017|46500.5|\n",
      "|   C|    83| Chennai|1999-02-28| 2018|52000.0|\n",
      "|   D|    60|  Mumbai|1989-06-13| 2016|51000.0|\n",
      "|   E|    92|Banglore|2000-11-15| 2016|52000.0|\n",
      "|   F|    69|  Mumbai|1995-10-12| 2017|75000.6|\n",
      "|   G|    84| Chennai|1998-11-25| 2016|64000.5|\n",
      "|   H|    71|Banglore|1994-09-15| 2018|52000.0|\n",
      "|   I|    77|     HYD|1996-01-15| 2017|46500.5|\n",
      "+----+------+--------+----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
